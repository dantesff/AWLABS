import gradio as gr
from roboflow import Roboflow
import supervision as sv
import cv2
import numpy as np
from PIL import Image
import tempfile
import os

rf = Roboflow(api_key="3xHadLJ7GhY5tMP7zbdE")

project_parts = rf.workspace().project("car-parts-segmentation")
model_parts = project_parts.version(2).model

project_damage = rf.workspace().project("car-damage-detection-ha5mm")
model_damage = project_damage.version(1).model


def detect_cleanliness(image_np):
    hsv = cv2.cvtColor(image_np, cv2.COLOR_RGB2HSV)
    saturation = hsv[:, :, 1].mean()
    value = hsv[:, :, 2].mean()
    gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)
    texture_variance = np.var(gray)

    is_dirty = False
    dirt_confidence = 0

    if saturation < 50 and value < 100:
        is_dirty = True
        dirt_confidence = 0.7
    elif texture_variance > 1000:
        is_dirty = True
        dirt_confidence = 0.6

    return is_dirty, dirt_confidence


def analyze_car(image):
    with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as tmp:
        image.save(tmp.name)
        img_path = tmp.name

    try:
        image_np = np.array(image)
        annotated_image = image_np.copy()

        result_damage = model_damage.predict(img_path, confidence=40).json()
        labels_damage = [item["class"] for item in result_damage["predictions"]]
        detections_damage = sv.Detections.from_inference(result_damage)

        mask_annotator = sv.MaskAnnotator()
        label_annotator = sv.LabelAnnotator(text_scale=0.5)

        if len(detections_damage.xyxy) > 0:
            annotated_image = mask_annotator.annotate(
                scene=annotated_image,
                detections=detections_damage
            )

            annotated_image = label_annotator.annotate(
                scene=annotated_image,
                detections=detections_damage,
                labels=labels_damage
            )

        damaged_parts = []
        if len(detections_damage.xyxy) > 0:
            x1, y1, x2, y2 = map(int, detections_damage.xyxy[0])
            cropped_damage = image_np[y1:y2, x1:x2]

            with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as damage_tmp:
                cv2.imwrite(damage_tmp.name, cropped_damage)
                damage_path = damage_tmp.name

            result_parts = model_parts.predict(damage_path, confidence=15).json()
            damaged_parts = list(set([item["class"] for item in result_parts["predictions"]]))

            os.unlink(damage_path)

        is_dirty, dirt_confidence = detect_cleanliness(image_np)
        cleanliness_state = "–ì—Ä—è–∑–Ω—ã–π" if is_dirty else "–ß–∏—Å—Ç—ã–π"

        cv2.putText(annotated_image, f"–ß–∏—Å—Ç–æ—Ç–∞: {cleanliness_state}",
                    (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        damage_count = len(detections_damage.xyxy)

        if damage_count > 0 and is_dirty:
            overall_state = "‚ö†Ô∏è –ü–ª–æ—Ö–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ (–ü–æ–≤—Ä–µ–∂–¥–µ–Ω–∏—è –∏ –≥—Ä—è–∑—å)"
        elif damage_count > 0:
            overall_state = "‚ö†Ô∏è –ü–æ–≤—Ä–µ–∂–¥–µ–Ω –Ω–æ —á–∏—Å—Ç—ã–π"
        elif is_dirty:
            overall_state = "‚ö†Ô∏è –ì—Ä—è–∑–Ω—ã–π –Ω–æ –±–µ–∑ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π"
        else:
            overall_state = "‚úÖ –û—Ç–ª–∏—á–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ"

        report = f"""
        üöó –û—Ç—á–µ—Ç –æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –∞–≤—Ç–æ–º–æ–±–∏–ª—è:

        üìä –û–±—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ: {overall_state}

        üîç –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π:
        - –ù–∞–π–¥–µ–Ω–æ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π: {damage_count}
        - –¢–∏–ø—ã –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π: {', '.join(labels_damage) if labels_damage else '–ù–µ—Ç'}
        - –ó–∞–¥–µ—Ç—ã–µ –¥–µ—Ç–∞–ª–∏: {', '.join(damaged_parts) if damaged_parts else '–ù–µ—Ç'}

        üßπ –û—Ü–µ–Ω–∫–∞ —á–∏—Å—Ç–æ—Ç—ã:
        - –°–æ—Å—Ç–æ—è–Ω–∏–µ: {cleanliness_state}
        - –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {dirt_confidence:.2f}
        - –ê–Ω–∞–ª–∏–∑: {'–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –≥—Ä—è–∑—å/–ø—ã–ª—å' if is_dirty else '–ê–≤—Ç–æ–º–æ–±–∏–ª—å —á–∏—Å—Ç—ã–π'}

        ‚ö†Ô∏è –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:
        {'‚Ä¢ –¢—Ä–µ–±—É–µ—Ç—Å—è –æ—Å–º–æ—Ç—Ä –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π' if damage_count > 0 else '‚Ä¢ –ü–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ'}
        {'‚Ä¢ –ê–≤—Ç–æ–º–æ–±–∏–ª—å –Ω—É–∂–¥–∞–µ—Ç—Å—è –≤ –º–æ–π–∫–µ' if is_dirty else '‚Ä¢ –ê–≤—Ç–æ–º–æ–±–∏–ª—å —á–∏—Å—Ç—ã–π'}
        {'‚Ä¢ –ü—Ä–æ–≤–µ—Ä—å—Ç–µ ' + ', '.join(damaged_parts) + ' –¥–ª—è —Ä–µ–º–æ–Ω—Ç–∞' if damaged_parts else ''}
        """

        return annotated_image, report

    finally:
        os.unlink(img_path)


css = """
.gradio-container {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    font-family: 'Arial', sans-serif;
}
.header {
    text-align: center;
    padding: 20px;
    background: white;
    border-radius: 15px;
    margin: 10px;
    box-shadow: 0 4px 6px rgba(0,0,0,0.1);
}
.column {
    background: white;
    padding: 20px;
    border-radius: 15px;
    margin: 10px;
    box-shadow: 0 4px 6px rgba(0,0,0,0.1);
}
button {
    background: linear-gradient(45deg, #FF6B6B, #FF8E53) !important;
    color: white !important;
    border: none !important;
    border-radius: 10px !important;
    padding: 15px 30px !important;
    font-size: 18px !important;
    font-weight: bold !important;
}
button:hover {
    background: linear-gradient(45deg, #FF8E53, #FF6B6B) !important;
    transform: translateY(-2px);
    box-shadow: 0 6px 12px rgba(0,0,0,0.2);
}
"""

with gr.Blocks(title="–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å–æ—Å—Ç–æ—è–Ω–∏—è –∞–≤—Ç–æ–º–æ–±–∏–ª—è", css=css, theme=gr.themes.Soft()) as demo:
    with gr.Column(elem_classes="header"):
        gr.Markdown("# üöó –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å–æ—Å—Ç–æ—è–Ω–∏—è –∞–≤—Ç–æ–º–æ–±–∏–ª—è")
        gr.Markdown("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–æ—Ç–æ –∞–≤—Ç–æ–º–æ–±–∏–ª—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π –∏ —á–∏—Å—Ç–æ—Ç—ã")

    with gr.Row():
        with gr.Column(elem_classes="column"):
            image_input = gr.Image(type="pil", label="üì∏ –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–æ—Ç–æ –∞–≤—Ç–æ–º–æ–±–∏–ª—è")
            analyze_btn = gr.Button("üîç –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∞–≤—Ç–æ–º–æ–±–∏–ª—å", variant="primary")

        with gr.Column(elem_classes="column"):
            image_output = gr.Image(label="üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞")
            text_output = gr.Textbox(label="üìù –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç", lines=18)

    analyze_btn.click(
        fn=analyze_car,
        inputs=image_input,
        outputs=[image_output, text_output]
    )

    with gr.Column(elem_classes="column"):
        gr.Markdown("""
        ### üìã –ß—Ç–æ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º:
        - **–ü–æ–≤—Ä–µ–∂–¥–µ–Ω–∏—è**: –¶–∞—Ä–∞–ø–∏–Ω—ã, –≤–º—è—Ç–∏–Ω—ã, –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏—è –∫—É–∑–æ–≤–∞
        - **–ß–∏—Å—Ç–æ—Ç—É**: –ì—Ä—è–∑—å, –ø—ã–ª—å, –∑–∞–≥—Ä—è–∑–Ω–µ–Ω–∏—è  
        - **–î–µ—Ç–∞–ª–∏**: –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        - **–û–±—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ**: –ü–æ–ª–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª—è
        """)

if __name__ == "__main__":
    demo.launch(share=True, server_name="0.0.0.0", server_port=7860)